<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Document</title>
    <style>
        .blue-underline {
            color: blue;  /* Задаем светло-голубой цвет текста */
            text-decoration: underline; /* Добавляем подчеркивание снизу */
        }
    </style>
</head>
<body>
    <h1>Сравнение открытых OLAP-систем
        Big Data: ClickHouse, Druid и Pinot </h1>

    <p>
        Оригинал - https://medium.com/@leventov/comparison-of-the-open-source-olap-systems-for-big-data-clickhouse-druid-and-pinot-8e042a5ed1c7
        <br>
        <br>
        <span class="blue-underline">ClickHouse</span>, 
        <span class="blue-underline">Druid</span> и 
        <span class="blue-underline">Pinot</span> – три открытых хранилища данных, которые позволяют выполнять аналитические запросы на больших объемах данных с интерактивными задержками. Эта статья - перевод 
        <a href="test">подробного сравнения</a>, выполненного Романом Левентовым.
    
        <h3> 
            Источники информации
        </h3>

        Подробности реализации <b>ClickHouse</b> стали мне известны от 
        <span class="blue-underline">Алексея Зателепина</span>, 
        одного <b>из ключевых разработчиков проекта</b>. Доступная на английском 
        документация достаточно скудна – наилучшим источником информации 
        служат последние четыре секции <span class="blue-underline">
        данной страницы документации</span>.
        <br><br>
        <b>Я сам участвую в развитии Druid</b>, но у меня нет личной заинтересованности в этой системе - по правде говоря, скорее всего в ближайшее время я перестану заниматься её разработкой. Поэтому читатели могут рассчитывать на отсутствие какой-либо предвзятости.
        <br><br>

        Всё, что я буду далее писать про <b>Pinot</b>, основывается на странице 
        <span class="blue-underline">Архитектура в вики Pinot</span>, а также на других страницах вики в разделе 
        “Проектная документация”. Последний раз они обновлялись в июне 2017 
        года - больше, чем полгода назад.
        <br><br>

        Рецензентами оригинальной статьи стали Алексей Зателепин и <span class="blue-underline">Виталий Людвиченко</span> 
        (разработчики ClickHouse), <span class="blue-underline">Жан Мерлино</span> (самый активный разработчик Druid),
        <span class="blue-underline">Кишор Гопалакришна</span>(архитектор Pinot) и <span class="blue-underline">Жан-Француа Им</span> (разработчик Pinot).
        Мы присоединяемся к благодарности автора и полагаем, что это многократно повышает
        авторитетность статьи. 
        <br><br>

        <b>Предупреждение</b>: статья достаточно большая, поэтому вполне возможно вы захотите ограничиться прочтением раздела “Заключение” в конце.
        <br><br>

        <h2> Сходства между системами </h2>
        <h3> Связанные данные и вычисления </h3>
        <b> На фундаментальном уровне, ClickHouse, Druid и Pinot похожи</b>, 
        поскольку они хранят данные и выполняют обработку запросов на одних и тех же узлах,
        уходя от “разъединенной” архитектуры BigQuery. Недавно я уже описывал 
        несколько наследственных проблем со связанной архитектурой в случае Druid [
        <span class="blue-underline">1, 2</span>].
        Открытого эквивалента для BigQuery на данный момент не существует 
        (за исключением, разве что, <span class="blue-underline">Drill</span>?) Возможным подходам к построению подобных 
        открытых систем посвящена <a href="test2">другая статье в моем блоге</a>.

        <h3>Отличия от Big Data SQL-систем: индексы и статическое распределение данных</h3>
        Рассматриваемые в этой статье системы <b>выполняют запросы быстрее</b>,
        чем системы Big Data из семейства класса SQL-on-Hadoop: Hive, Impala, Presto и Spark, даже когда последние получают доступ к данным, хранящимся в колоночном формате - к примеру, Parquet или Kudu. Это происходит потому, что в ClickHouse, Druid и Pinot:
        <ul>
            <li>Имеется <b>свой собственный формат для хранения данных с индексами</b>, и они тесно интегрированы с движками обработки запросов. Системы класса SQL-on-Hadoop обычно можно назвать агностиками относительно форматов данных и поэтому они менее “навязчивы” в бэкендах Big Data.</li>
            <li><b>Данные распределены относительно “статично”</b> между узлами, и при распределенном выполнении запроса это можно использовать. Обратная сторона медали при этом в том, что ClickHouse, Druid и Pinot <b>не поддерживают запросы, которые требуют перемещения большого количества данных</b> между узлами - к примеру, join между двумя большими таблицами.</li>
        </ul>

        <h3> Отсутствие точечных обновлений и удалений</h3>
        Находясь на другой стороне спектра баз данных, ClickHouse, Druid и Pinot
        <b>не поддерживают точечные обновления и удаления</b>, в противоположность колоночным
        системам вроде Kudu, InfluxDB и Vertica (?). Это даёт ClickHouse, Druid и Pinot
        возможность производить более эффективное колоночное сжатие и более агрессивные
        индексы, что означает <b>большую эффективность использования ресурсов </b>и
        быстрое выполнение запросов.
        <br><br>

        Разработчики ClickHouse в Yandex планируют начать поддерживать <span class="blue-underline">обновления и удаления в будущем</span>, но я не уверен, будут ли это “настоящие” точечные запросы или обновления/удаления диапазонов данных.
        <h3>Поглощение в стиле Big Data</h3>
        Все три системы поддерживают потоковое поглощение данных из Kafka. Druid и Pinot поддерживают потоковую передачу данных стриминг в <span class="blue-underline">Лямбда-стиле</span> и пакетное поглощение одних и тех же данных. ClickHouse поддерживает пакетные вставки напрямую, поэтому ему не требуется отдельная система пакетного поглощения подобная той, что используется в Druid и Pinot. Если вас интересуют подробности, то их вы сможете найти далее.
        <h3>Проверено на крупном масштабе</h3>
        Все три системы проверены на работоспособность в крупных масштабах: в <span class="blue-underline">работает кластер ClickHouse</span>, состоящий из примерно десятка тысяч ядер CPU. В Metamarkets используется <span class="blue-underline">кластер Druid аналогичного размера</span>. Один кластер Pinot в LinkedIn включает в себя <span class="blue-underline"> "тысячи машин”</span>.
        <h3>Незрелость</h3>
        Все рассматриваемые в статье системы являются <b>незрелыми по меркам открытых enterprise-систем Big Data</b>. Однако, скорее всего они незрелы не более, чем среднестатистическая открытая система Big Data - но это совсем другая история. В ClickHouse, Druid и Pinot недостает некоторых очевидных оптимизаций и функциональности, и они кишат багами (насчет ClickHouse и Pinot я не уверен на все 100%, но не вижу причин, по которым они в этом плане были бы лучше Druid).
        Это плавно подводит нас к следующему важному разделу.
        
        <h2>Про сравнение производительности и выбор системы</h2>
        
        Я регулярно вижу в сети, как некоторые проводят сравнения систем больших данных: они берут набор своих данных, каким-либо образом “скармливают” его оцениваемой системе, а затем немедленно пытаются измерить производительность - сколько памяти или дискового пространства было занято, и насколько быстро выполнялись запросы. Причем понимание того, как устроены изнутри испытываемые ими системы, у них отсутствует. Затем, используя лишь подобные специфичные данные о производительности - иногда вместе со списками функциональности, которая им нужна и которая есть в системе <i>на настоящий момент</i>, - они в итоге делают свой выбор или, что еще хуже, выбирают написать свою собственную, “лучшую” систему с нуля.
        <br><br>
        Такой подход мне кажется неправильным, по крайней мере он неприменим в отношении открытых OLAP-систем для Big Data. Задача создания системы Bid Data OLAP, которая смогла бы работать эффективно в большинстве сценариев использования и содержала бы все необходимые функции настолько велика, что я оцениваю ее реализацию как минимум в <b>100 человеко-лет</b>.
        <br><br>
        На сегодня, ClickHouse, Druid и Pinot оптимизированы только для конкретных сценариев использования, которые требуются их разработчиком - и содержат по большей части лишь те функции, в которых нуждаются сами разработчики. Я могу гарантировать, что ваш случай обязательно “упрется” в те узкие места, с которыми разработчики рассматриваемых OLAP-систем еще не сталкивались - или же в те места, что их не интересуют. 
        <br><br>
        Не говоря уже о том, что упомянутый выше подход “забросить данные в систему, о которой вы ничего не знаете, и затем измерить её эффективность” весьма вероятно даст искаженный результат из-за серьезных “узких” мест, которые на самом деле могли бы быть исправлены <b> простым изменением конфигурации</b>, схемы данных или другим построением запроса.
        
        <h3>CloudFlare: ClickHouse против Druid</h3>
        Одним таким примером, хорошо иллюстрирующим описанную выше проблему, является пост Марека Вавруша о <span class="blue-underline"> между ClickHouse и Druid в Cloudflare</span>. Им потребовалось 4 сервера ClickHouse (которые со временем превратились в 9), и по их оценкам, для разворачивания аналогичной установки Druid им бы потребовались “сотни узлов”. Пусть Марек и признает, что сравнение является нечестным, поскольку Druid недостаёт “сортировки по первичному ключу”, он возможно даже не осознает, что достичь примерно того же самого эффекта в Druid возможно просто <span class="blue-underline">установив правильный порядок измерений в <i>“ingestion spec”</i></span> и произведя простую подготовку данных: обрезать значение колонки __time в Druid до некоей грубой детализации (к примеру, один час) и опционально добавить другую “длинно-типовую” колонку “precise_time”, если для некоторых запросов требуются более точные временные рамки. Да, это хак, но, как мы только что выяснили, и в Druid можно сортировать данные по какому-либо измерению перед __time, и это достаточно просто реализовать.
        <br><br>
        Впрочем, я не стану спорить с их итоговым решением выбрать ClickHouse, поскольку на масштабе примерно в 10 узлов и для их нужд ClickHouse мне тоже кажется лучшим выбором, чем Druid. Но сделанное ими заключение о том, что ClickHouse как минимум на порядок эффективнее (по меркам стоимости инфраструктуры), чем Druid - это серьезное заблуждение. На самом деле, из рассматриваемых нами сегодня систем, <b>Druid предлагает наилучшую возможность для реально дешевых установок</b> (смотрите раздел “Уровни узлов обработки запросов в Druid ” ниже).
        <blockquote> <mark>Когда вы выбираете систему OLAP Big Data, не сравнивайте то, насколько они сейчас хорошо подходят для вашего случая. Сейчас они все субоптимальны. Вместо этого, сравните, насколько быстро ваша компания способна заставить двигаться эти системы в том направлении, которое нужно именно вам.</mark></blockquote>
        В силу своей фундаментальной архитектурной схожести, ClickHouse, Druid и Pinot имеют примерно один и тот же “предел” эффективности и оптимизации производительности. Здесь нет “волшебной таблетки”, которая позволила бы какой-либо из этих систем быть быстрее, чем остальные. Не позволяйте запутать себя тем фактом, что в своем текущем состоянии системы показывают себя очень по-разному в различных бенчмарках. 
        <br><br>
        Допустим, Druid не поддерживает “сортировку по первичному ключу” настолько хорошо, насколько это умеет ClickHouse - а ClickHouse в свою очередь не поддерживает “инвертированные индексы” столь же хорошо, как Druid, что дает данным системам преимущества с той или иной нагрузкой. <b>Упущенные оптимизации могут быть реализованы в выбранной системе при помощи не таких уж и больших усилий</b>, если у вас есть намерение и возможность решиться на подобный шаг.
        <ul>
            <li>В вашей организации должны быть инженеры, способные прочитать, понять и модифицировать исходный код выбранной системы, к тому же у них должно быть на это время. Заметьте, что ClickHouse написан на C++, а Druid и Pinot —  на Java.</li>
            <li>Или же ваша организация должна подписать контракт с компанией, которая оказывает поддержку выбранной системы. Это будут <span class="blue-underline">Altinity</span> для ClickHouse, <span class="blue-underline">Imply</span> и <span class="blue-underline">Hortonworks</span> для Druid. Для Pinot таких компаний в данный момент нет.</li>
        </ul>
        Другие сведения о разработке систем, которые вам стоит принять во внимание:
        <ul>

        <li>Авторы ClickHouse, работающие в Yandex, утверждают, что они тратят 50% своего времени на создание функциональности, которая требуется им внутри компании, и другие 50% уходят на функции, который набирают большинство “голосов сообщества”. Однако, чтобы вы получили от этого факта преимущество, требуется, чтобы <b>функции, которые нужны вам, были и наиболее востребованы сообществом</b> ClickHouse.</li>
        <li>Разработчики Druid из Imply мотивированы работать над широко используемыми функциями, поскольку это позволит им максимально увеличить объем охвата своего бизнеса в будущем.</li>
        <li>Процесс разработки Druid сильно напоминает <span class="blue-underline">модель Apache</span>, когда ПО несколько лет разрабатывается несколькими компаниями, у каждой из который достаточно своеобразные и различные приоритеты, и среди них нет ведущей компании. ClickHouse и Pinot пока еще далеки от подобного этапа, поскольку ими занимаются соответственно лишь Yandex и Linkedin. Сторонний вклад в развитие Druid имеет минимальный шанс быть отклоненным в силу того, что он расходится с видением основного разработчика - ведь <b>в Druid нет “основной” компании-разработчика</b>.</li>
        <li>Druid поддерживает “API разработчика”, который позволяет привносить собственные типы колонок, механизмы агрегации, возможные варианты для «глубокого хранения» и пр., причем все это вы можете держать в кодовой базе, отдельной от самого ядра Druid. Данное API документировано разработчиками Druid, и они следят за его совместимостью с предыдущими версиями. Однако, оно недостаточно “взрослое”, и ломается практически с каждым новым релизом Druid. Насколько мне известно, в ClickHouse и Pinot схожие API не поддерживаются.</li>
        <li>Согласно Github, <b>над Pinot работает наибольшее число людей</b> - по всей видимости, лишь за прошлый год в Pinot было вложено <a href="test3">не менее 10 человеко-лет</a>. Для ClickHouse эта цифра составляет примерно 6 человеко-лет, а для Druid - 7. В теории, это должно означать, что Pinot улучшается быстрее всех остальных систем, которые мы рассматриваем.
        </ul>
        Архитектуры Druid и Pinot почти что идентичны друг другу, в то время как ClickHouse стоит слегка в стороне. Поэтому сначала мы сравним ClickHouse c “обобщенной” архитектурой Druid/Pinot, а затем обсудим мелкие различия между Druid и Pinot.
        
        <h2>Различия между ClickHouse и Druid/Pinot</h2>
        <h3>Управление данными: Druid и Pinot</h3>
        В Druid и Pinot, все данные в каждой “таблице” (как бы она не называлась в терминологии этих систем) разбиваются на указанное количество частей. По временой оси, данные обычно разделены с заданым интервалом. Затем эти части данных «запечатываются» индивидуально в самостоятельные автономные сущности, называемые «сегментами». Каждый сегмент включает в себя метаданные таблицы, сжатые столбчатые данные и индексы.
        <br><br>
        Сегменты хранятся в файловой системе хранилища «глубокого хранения» (например, HDFS) и могут быть загружены на узлы обработки запросов, но последние не отвечают за устойчивость сегментов, поэтому узлы обработки запросов могут быть заменены относительно свободно. <b>Сегменты не привязаны жестко к конкретным узлам</b>и могут быть загружены на те или другие узлы. Специальный выделенный сервер (который называется “координатором” в Druid и “контроллером” в Pinot, но я ниже обращаюсь к нему как к “мастеру”) отвечает за присвоение сегментов узлам, и перемещению сегментов между узлами, если потребуется.
        <br><br>
        Это не противоречит тому, что я отмечал выше, все три системы имеют статическое распределение данных между узлами, поскольку загрузки сегментов и их перемещения в Druid - и насколько я понимаю в Pinot - являются дорогими операциями и потому не выполняются для каждой отдельной очереди, а происходят обычно раз в несколько минут/часов/дней.
        <br><br>
        Метаданные сегментов хранятся в ZooKeeper - напрямую в случае Druid, и при помощи фреймворка <a href="test4">Helix</a> в Pinot. В Druid метаданные также хранятся в базе SQL, об этом будет подробнее в разделе “Различия между Druid и Pinot”.
    
        <h3>Управление данными: ClickHouse</h3>
        В ClickHouse нет “сегментов”, содержащих данные, попадающие в конкретные временные диапазоны. В нем нет “глубокого хранения” для данных, узлы в кластере ClickHouse также отвечают и за обработку запросов, и за постоянство/устойчивость данных, хранящихся на них. Так что вам <b> не потребуется HDFS </b> или облачное хранилище данных вроде Amazon S3.
        <br><br>
        В ClickHouse имеются секционированные таблицы, состоящие из указанного набора узлов. Здесь нет “центральной власти” или сервера метаданных. Все узлы, между которыми разделена та или иная таблица, содержат полные, идентичные копии метаданных, включая адреса всех остальных узлов, на которых хранятся секции этой таблицы.
        <br><br>
        Метаданные секционированной таблицы включают “весы” узлов для распределения свежезаписываемых данных - к примеру, 40% данных должны идти на узел A, 30% на узел B и 30% на C. Обычно же распределение должно происходить равномерно, “перекоос”, как в этом примере, требуется только тогда, когда к секционированной таблице добавляется новый узел и нужно побыстрее заполнить его какими-либо данными. <b>Обновления этих “весов” должны выполняться вручную</b> администраторами кластера ClickHouse, или же автоматизированной системой, построенной поверх ClickHouse.

        <h3>Управление данными: сравнение </h3>
        Подход к управлению данными в ClickHouse проще, чем в Druid и Pinot: не требуется “глубокого хранилища”, всего один тип узлов, не требуется выделенного сервера для управления данными. Но подход ClickHouse приводит к некоторым трудностям, когда любая таблица данных вырастает настолько большой, что требуется ее разбиение между десятком или более узлов: коэффициент усиления запроса становится настолько же велик, насколько и фактор секционирования - даже для запросов, которые покрывают небольшой интервал данных:
        <br><br> 
    
        <!-- img -->

        В примере, показанном на изображении выше, данные таблицы распределены между тремя узлами в Druid/Pinot, но запрос по малому интервалу данных обычно затрагивает лишь два из них (до той поры, пока интервал не пересечет пограничный интервал сегмента). В ClickHouse, любые запросы будут вынуждены затронуть три узлв – если таблица сегментирована между тремя узлами. В данном примере разница не выглядит настолько существенно, однако представьте себе, что случится, если число узлов достигнет 100 – в то время как фактор сегментирования по-прежнему может быть равен, например, 10 в Druid/Pinot.
        <br><br>
        Чтобы смягчить эту проблему, самый большой кластер ClickHouse в Яндексе, состоящий из сотен узлов, в действительности разбит на многие «под-кластеры» с несколькими десятками узлов в каждом. Кластер ClickHouse используется в работе с аналитикой по веб-сайтам, и каждая точка данных имеет измерение «ID вебсайта». Существует жесткая привязка каждого ID сайта к конкретному под-кластеру, куда идут все данные для этого идентификатора сайта. Поверх кластера ClickHouse есть слой бизнес-логики, который управляет этим разделением данных при поглощении данных и выполнении запросов. К счастью, в их сценариях использования совсем немного запросов затрагивают несколько идентификаторов сайтов, и подобные запросы идут не от пользователей сервиса, поэтому у них нет жесткой привязки к реальному времени согласно соглашению об уровне услуг.
        <br><br>
        Другим недостатком подхода ClickHouse является то, что, когда кластер растет очень быстро, данные не могут перебалансироваться автоматически без участия человека, который вручную поменяет «веса» узлов в разбиваемой таблице.

        <h3>Уровни узлов обработки запросов в Druid</h3>
        Управление данными при помощи сегментов «проще себе представить» - эта концепция хорошо ложится на наши когнитивные способности. Сами сегменты можно перемещать между узлами относительно просто. Эта две причины позволили Druid реализовать “разделение на уровни” узлов, занимающихся обработкой запросов: старые данные автоматически перемещаются на сервера с относительно большими дисками, но меньшим количеством памяти и CPU, что позволяет значительно снизить стоимость большого рабочего кластера Druid за счет замедления запросов к более старым данным.
        <br><br>
        Эта функция позволяет Metamarkets экономить сотни тысяч долларов расходов на инфраструктуру Druid каждый месяц - в противовес тому варианту, если бы использовался “плоский” кластер.

        <!-- img -->

        Насколько мне известно, в ClickHouse и Pinot пока еще нет похожей функциональности - предполагается, что все узлы в их кластерах одинаковы.
        <br><br>
        В силу того, что архитектура Pinot весьма схожа с архитектурой Druid, как мне кажется, будет не слишком сложно добавить аналогичную функцию в Pinot. Тяжелее будет в случае с ClickHouse, поскольку для реализации данной функции крайне полезно использование концепта “сегментов”, однако это всё равно возможно.
    
        <h3>Репликация данных: Druid и Pinot</h3>
        Единицей репликации в Druid и Pinot является единичный сегмент. Сегменты реплицируются на уровне «глубокого хранения» (например, в три реплики на HDFS, или при помощи хранилища BLOB-объектов в Amazon S3), и на уровне обработки запросов: обычно и в Druid и в Pinot, каждый сегмент загружается на два различных узла. «Мастер»-сервер мониторит уровни репликации для каждого сегмента и загружает сегмент на какой-либо сервер, если фактор репликации падает ниже заданного уровня (например, если какой-либо из узлов перестаёт отвечать).
        
        <h3>Репликация данных: ClickHouse</h3>
        Единицей репликации в ClickHouse является секция таблицы на сервере (например, все данные из какой-либо таблицы, хранящиеся на сервере). Аналогично секционированию, репликация в ClickHouse является скорее «статической и конкретной», чем «в облачном стиле»: несколько серверов знают, что они являются репликами друг друга (для некоторой конкретной таблицы; в случае другой таблицы, конфигурация репликации может отличаться). Репликация предоставляет и устойчивость, и доступность запросов. Когда повреждается диск на одном узле, данные не теряются, поскольку они хранятся еще и на другом узле. Когда какой-либо узел временно недоступен, запросы могут быть перенаправлены на реплику.
        <br><br>
        В самом большом кластере ClickHouse в Яндексе есть два одинаковых набора узлов в различных дата-центрах, и они спарены. В каждой паре узлы являются репликами друг друга (используется фактор репликации, равный двум), и они расположены в различных дата-центрах.
        <br><br>
        ClickHouse полагается на ZooKeeper для управления репликацией – поэтому, если вам не нужна репликация, то вам не нужен и ZooKeeper. Это означает, что ZooKeeper не потребуется и для ClickHouse, развернутого на одиночном узле.
        
        <h3>Поглощение данных: Druid и Pinot</h3>
        В Druid и Pinot узлы обработки запросов специализируются на загрузке сегментов и обслуживают запросы к данным в сегментах; они не занимаются накоплением новых данных и производством новых сегментов.
        <br><br>
        Когда таблица может обновляться с задержкой в час или более, сегменты создаются при помощи движков пакетной обработки – к примеру, Hadoop или Spark. И в Druid, и в Pinot есть первоклассная поддержка Hadoop из коробки. Существует <a href="test5"> сторонний плагин для поддержки индексации Druid в Spark</a>, но в данный момент официально он не поддерживается. Насколько мне известно, в Pinot такого уровня поддержки Spark пока нет, то есть вы должны быть готовы разобраться с интерфейсами Pinot и кодом, а затем самостоятельно написать код на Java/Scala, пусть это и не должно быть слишком сложно. (Впрочем, с момента публикации оригинальной статьи поддержка Spark в Pinot <span class="blue-underline">была внесена контрибьютором</span>).
        <br><br>
        Когда таблица должна обновляться в реальном времени, здесь приходит на помощь идея “реалтаймовых» узлов, которые делают три вещи: принимает новые данные из Kafka (Druid поддерживает и другие источники), обслуживает запросы с недавними данными, создает сегменты в фоне и затем записывает их в “глубокое хранилище”.
        
        <h3>Поглощение данных: ClickHouse</h3>
        Тот факт, что ClickHouse не требуется готовить “сегменты”, содержащие все данные и попадающие в заданные временные интервалы, позволяет строить более простую архитектуру поглощения данных. ClickHouse не требуется ни пакетный движок обработки вроде Hadoop, ни “реалтаймовые” узлы. Обычные узлы ClickHouse - те же самые, что занимаются хранением данных и обслуживают запросы к ним - напрямую принимают пакетные записи данных.
        <br><br>
        Если таблица разбита на сегменты, то узел, который принимает пакетную запись (например, 10к строк) распределяет данные согласно “весам” (смотрите раздел ниже). Строки записываются одним пакетом, который формирует небольшое “множество”. Множество немедленно конвертируется в колоночный формат. На каждом узле ClickHouse работает фоновый процесс, который объединяет наборы строк в еще большие наборы. Документация ClickHouse сильно завязана на принцип, известный как “MergeTree”, и подчеркивает схожесть его работы с <span class="blue-underline">LSM-деревом</span>, хотя меня это слегка смущает, поскольку данные не организованы в деревья - они лежат в плоском колончатом формате.
        
        <h3>Поглощение данных: сравнение</h3>
        Поглощение данных в Druid и Pinot является “тяжелым”: оно состоит из нескольких различных сервисов, и управление ими - это тяжелый труд.
        <br><br>
        Поглощение данных в ClickHouse гораздо проще (что компенсируется сложностью управления “историческими” данными - т.е. данными не в реальном времени), но и здесь есть один момент: вы должны иметь возможность собирать данные в пакеты до самого ClickHouse. Автоматическое поглощение и пакетный сбор данных из <span class="blue-underline">Kafka доступно “из коробки”</span>>, но если у вас используется другой источник данных в реальном времени (здесь подразумевается всё, что угодно, в диапазоне между инфраструктурой запросов, альтернативной Kafka, и стриминговых движков обработки, вплоть до различных HTTP-endpoint), то вам придется создать промежуточный сервис по сбору пакетов, или же внести код напрямую в ClickHouse.
        
        <h3>Выполнение запроса</h3>
        В <b>Druid и Pinot</b> имеется отдельный слой узлов, называемых “брокерами”, которые принимают все запросы к системе. Они определяют, к каким “историческим” (<i>содержащим данные не в реальном времени</i>) узлам обработки запросов должны быть отправлены подзапросы, основываясь на отображении сегментов в узлы, в которых сегменты загружаются. Брокеры хранят информацию об отображении в памяти. Брокер-узлы отправляют дальше подзапросы к узлам обработки запросов, и когда результаты этих подзапросов возвращаются, брокер объединяет их и возвращает финальный комбинированный результат пользователю.
        <br><br>
        Я не берусь предполагать, зачем при проектировании Druid и Pinot было принято решение о введении еще одного типа узлов. Однако, теперь они кажутся их неотъемлемой частью, поскольку, когда общее количество сегментов в кластере начинает превышать десять миллионов, информация об отображении сегментов в узлы начинает занимать гигабайты памяти. Это очень расточительно – выделять столько много памяти на каждом узле для обработки запросов. Вот вам и еще один недостаток, который накладывается на Druid и Pinot их «сегментированной» архитектурой управления данными.
        <br><br>
        В <b>ClickHouse</b> выделять отдельный набор узлов под “брокер запросов” обычно не требуется. Существует специальный, эфемерный <span class="blue-underline">“распределенный” тип таблицы</span> в ClickHouse, который может быть установлен на любом узле, и запросы к этой таблице будут делать все то же, за что отвечают брокер-узлы в Druid и Pinot. Обычно подобные эфемерные таблицы размещаются на каждом узле, который участвует в секционированной таблице, так что на практике каждый узел может быть “входной точкой” для запроса в кластер ClickHouse. Этот узел может выпускать необходимые подзапросы к другим секциями, обрабатывать свою часть запроса самостоятельно и затем объединять её с частичными результатами от других секций.
        <br><br>
        Когда узел (или один из процессинговых узлов в ClickHouse, или брокер-узел в Druid и Pinot) выпускает подзапросы к другим, и один или несколько подзапросов по какой-либо причине заканчиваются неудачей, ClickHouse и Pinot обрабатывают эту ситуацию правильно: они объединяют результаты успешно выполненных подзапросов вместе, и всё равно возвращают частичный результат пользователю. <span class="blue-underline"> Druid этой функции сейчас очень недостает</span>: если в нем выполнение подзапроса заканчивается неудачей, то неудачей закончится и весь запрос целиком.

        <h3>ClickHouse vs. Druid или Pinot: Выводы</h3>
        “Сегментированный” подход к управлению данными в Druid и Pinot против более простого управления данными в ClickHouse определяет многие аспекты систем. Однако, важно заметить, что это различие оказывает небольшое (или не оказывает вовсе) влияние на потенциальную эффективность сжатия (впрочем, история про компрессию для всех трех систем имеет печальный конец по нынешнему состоянию дел), или на скорость обработки запросов.
        <br><br>
        <b>ClickHouse</b> похож на традиционные RDMBS, например, PostgreSQL. В частности, ClickHouse можно развернуть на всего один сервер. Если планируемый размер невелик - скажем, не больше порядка 100 ядер CPU для обработки запросов и 1 TB данных, я бы сказал, что ClickHouse имеет значительные преимущества перед Druid и Pinot в силу своей простоты и отсутствия необходимости в дополнительных типах узлов, таких как “мастер”, “узлы поглощения в реальном времени”, “брокеры”. На этом поле, ClickHouse соревнуется скорее с InfluxDB, чем с Druid или Pinot.
        <br><br>
        <b>Druid and Pinot</b> похож на системы Big Data вроде HBase. Здесь в виду имеются не характеристики производительности, а зависимость от ZooKeper, зависимость от персистентного реплицируемого хранилища (к примеру, HDFS), сосредоточение внимания на устойчивости к отказам отдельных узлов, а также автономная работа и управление данными, не требующими постоянного внимания человека.
        <br><br>
        Для широкого спектра приложений, ни ClickHouse, ни Druid или Pinot не являются очевидными победителями. В первую очередь, вы должны принимать во внимание вашу способность разобраться с исходным кодом системы, исправлять баги, добавлять новые функции и т.д. Это подробнее обсуждается в разделе “Про сравнение производительности и выбор системы”.
        <br><br>
        Во-вторых, вам стоит взглянуть на таблицу ниже. Каждая ячейка в этой таблице описывает свойство приложения, которое позволит определить предпочтительную систему. Строки отсортированы не в порядке важности. Важность различных свойств может разниться от приложения к приложению, но в целом можно применить следующий подход: если ваше приложение соответствует подавляющему большинству строк со свойствами в одной из колонок, то относящаяся к ней система в вашем случае является предпочтительным выбором.
        <br><br>

    </p>
    
</body>
</html>